{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywdiUei4CFQC",
        "outputId": "83fcbd9f-55cf-4e9c-c623-63147061f731"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "  Using cached gensim-4.3.3.tar.gz (23.3 MB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Using cached numpy-1.26.4.tar.gz (15.8 MB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'error'\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [21 lines of output]\n",
            "      + c:\\Users\\danny\\rnn_258\\Scripts\\python.exe C:\\Users\\danny\\AppData\\Local\\Temp\\pip-install-75d82o3f\\numpy_5774dff1de5f48ecac9648176d92d521\\vendored-meson\\meson\\meson.py setup C:\\Users\\danny\\AppData\\Local\\Temp\\pip-install-75d82o3f\\numpy_5774dff1de5f48ecac9648176d92d521 C:\\Users\\danny\\AppData\\Local\\Temp\\pip-install-75d82o3f\\numpy_5774dff1de5f48ecac9648176d92d521\\.mesonpy-_hvpcy3m -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\danny\\AppData\\Local\\Temp\\pip-install-75d82o3f\\numpy_5774dff1de5f48ecac9648176d92d521\\.mesonpy-_hvpcy3m\\meson-python-native-file.ini\n",
            "      The Meson build system\n",
            "      Version: 1.2.99\n",
            "      Source dir: C:\\Users\\danny\\AppData\\Local\\Temp\\pip-install-75d82o3f\\numpy_5774dff1de5f48ecac9648176d92d521\n",
            "      Build dir: C:\\Users\\danny\\AppData\\Local\\Temp\\pip-install-75d82o3f\\numpy_5774dff1de5f48ecac9648176d92d521\\.mesonpy-_hvpcy3m\n",
            "      Build type: native build\n",
            "      Project name: NumPy\n",
            "      Project version: 1.26.4\n",
            "      WARNING: Failed to activate VS environment: Could not find C:\\Program Files (x86)\\Microsoft Visual Studio\\Installer\\vswhere.exe\n",
            "      \n",
            "      ..\\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]\n",
            "      The following exception(s) were encountered:\n",
            "      Running `icl \"\"` gave \"[WinError 2] The system cannot find the file specified\"\n",
            "      Running `cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
            "      Running `cc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
            "      Running `gcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
            "      Running `clang --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
            "      Running `clang-cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
            "      Running `pgcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
            "      \n",
            "      A full log can be found at C:\\Users\\danny\\AppData\\Local\\Temp\\pip-install-75d82o3f\\numpy_5774dff1de5f48ecac9648176d92d521\\.mesonpy-_hvpcy3m\\meson-logs\\meson-log.txt\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "error: metadata-generation-failed\n",
            "\n",
            "× Encountered error while generating package metadata.\n",
            "╰─> See above for output.\n",
            "\n",
            "note: This is an issue with the package mentioned above, not pip.\n",
            "hint: See above for details.\n"
          ]
        }
      ],
      "source": [
        "#%pip install gensim\n",
        "# %pip install torch\n",
        "# %pip install pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cwEPu8f73JeW"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import word2vec\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import defaultdict\n",
        "import ast\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import string\n",
        "import tempfile\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlOdV2Mn6A1k",
        "outputId": "bdd02b21-9850-4fac-9587-e5a25c15d1eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\danny\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\danny\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\danny\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQe9obNmQQP_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "NVIDIA GeForce RTX 3070\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzLNudar_sPi"
      },
      "outputs": [],
      "source": [
        "def build_vocab(sentences):\n",
        "    word_counts = Counter(itertools.chain(*sentences))\n",
        "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
        "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
        "    return word_counts, vocabulary, vocabulary_inv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy2ycN9oCreK"
      },
      "outputs": [],
      "source": [
        "# A function used to learn word embeddings through Word2vec module\n",
        "def get_embeddings(inp_data, vocabulary_inv, size_features=100,\n",
        "                   mode='skipgram',\n",
        "                   min_word_count=2,\n",
        "                   context=5):\n",
        "    model_name = \"embedding\"\n",
        "    model_name = os.path.join(model_name)\n",
        "    num_workers = 15\n",
        "    downsampling = 1e-3\n",
        "    print('Training Word2Vec model...')\n",
        "    sentences = [[vocabulary_inv[w] for w in s] for s in inp_data]\n",
        "    if mode == 'skipgram':\n",
        "        sg = 1\n",
        "        print('Model: skip-gram')\n",
        "    elif mode == 'cbow':\n",
        "        sg = 0\n",
        "        print('Model: CBOW')\n",
        "    embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,\n",
        "                                        sg=sg,\n",
        "                                        vector_size=size_features,\n",
        "                                        min_count=min_word_count,\n",
        "                                        window=context,\n",
        "                                        sample=downsampling)\n",
        "    print(\"Saving Word2Vec model {}\".format(model_name))\n",
        "    embedding_weights = np.zeros((len(vocabulary_inv), size_features))\n",
        "    for i in range(len(vocabulary_inv)):\n",
        "        word = vocabulary_inv[i]\n",
        "        if word in embedding_model.wv:\n",
        "            embedding_weights[i] = embedding_model.wv[word]\n",
        "        else:\n",
        "            embedding_weights[i] = np.random.uniform(-0.25, 0.25,\n",
        "                                                     embedding_model.vector_size)\n",
        "    return embedding_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5M4Ag-VCtFe"
      },
      "outputs": [],
      "source": [
        "def preprocess_df(df):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stop_words.add('would')\n",
        "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
        "    preprocessed_sentences = []\n",
        "    for i, row in df.iterrows():\n",
        "        sent = row[\"text\"]\n",
        "        sent_nopuncts = sent.translate(translator)\n",
        "        words_list = sent_nopuncts.strip().split()\n",
        "        filtered_words = [word for word in words_list if word not in stop_words and len(word) != 1] # also skip space from above translation\n",
        "        preprocessed_sentences.append(\" \".join(filtered_words))\n",
        "    df[\"text\"] = preprocessed_sentences\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_predictions(model, dataloader, device, output_path):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for text, state_idx, feats in dataloader:\n",
        "            text, state_idx, feats = text.to(device), state_idx.to(device), feats.to(device)\n",
        "            outputs = model(text, state_idx, feats)\n",
        "            predicted = torch.argmax(outputs, dim=1)\n",
        "            preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    df_out = pd.DataFrame({'label': preds})\n",
        "    df_out.to_csv(output_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CKV5vtoKKzk"
      },
      "outputs": [],
      "source": [
        "class RestaurantRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, state_vocab_size, embed_dim, state_embed_dim, hidden_dim, num_structured_feats, num_classes, embedding_weights=None):\n",
        "        super(RestaurantRNN, self).__init__()\n",
        "        self.word_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        if embedding_weights is not None:\n",
        "            self.word_embedding.weight.data.copy_(torch.tensor(embedding_weights))\n",
        "        \n",
        "        self.state_embedding = nn.Embedding(state_vocab_size, state_embed_dim)\n",
        "        self.rnn = nn.GRU(embed_dim, hidden_dim, num_layers=2, batch_first=True)\n",
        "\n",
        "        total_input_dim = hidden_dim + state_embed_dim + num_structured_feats\n",
        "        self.fc = nn.Linear(total_input_dim, num_classes)\n",
        "\n",
        "    def forward(self, text_input, state_input, structured_feats):\n",
        "        embedded_text = self.word_embedding(text_input) \n",
        "        _, hidden = self.rnn(embedded_text)\n",
        "        hidden = hidden[-1]\n",
        "\n",
        "        state_emb = self.state_embedding(state_input)\n",
        "\n",
        "        combined = torch.cat((hidden, state_emb, structured_feats), dim=1)\n",
        "        output = self.fc(combined)\n",
        "        return output\n",
        "\n",
        "class RestaurantDataset(Dataset):\n",
        "    def __init__(self, text_seqs, state_idxs, structured_feats, labels=None, id=None):\n",
        "        self.text_seqs = torch.tensor(text_seqs, dtype=torch.long)\n",
        "        self.state_idxs = torch.tensor(state_idxs, dtype=torch.long)\n",
        "        self.structured_feats = torch.tensor(structured_feats, dtype=torch.float32)\n",
        "        self.id = id\n",
        "        \n",
        "        if labels is not None:\n",
        "            self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "        else:\n",
        "            self.labels = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels) if self.labels is not None else len(self.text_seqs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.labels is not None:\n",
        "            return (\n",
        "                self.text_seqs[idx], \n",
        "                self.state_idxs[idx], \n",
        "                self.structured_feats[idx], \n",
        "                self.labels[idx]\n",
        "            )\n",
        "        else:\n",
        "            # For test set, no label\n",
        "            return (\n",
        "                self.text_seqs[idx], \n",
        "                self.state_idxs[idx], \n",
        "                self.structured_feats[idx],\n",
        "            )\n",
        "    \n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How many epochs to wait after last improvement.\n",
        "            min_delta (float): Minimum change to qualify as an improvement.\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = float('inf')\n",
        "        self.early_stop = False\n",
        "        self.best_model_state = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            self.best_model_state = model.state_dict()\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def safe_state_transform(state_list, le, unk_idx):\n",
        "    idxs = []\n",
        "    for s in state_list:\n",
        "        try:\n",
        "            idxs.append(le.transform([s])[0])\n",
        "        except ValueError:\n",
        "            idxs.append(unk_idx)\n",
        "    return idxs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "id": "b8H7YrXCCuzY",
        "outputId": "001c4e5a-83b8-4538-fb7b-82d2b1f2423a"
      },
      "outputs": [],
      "source": [
        "data_path = \"C:/Users/danny/OneDrive/Documents/UCSD/DSC 258R/kaggle_proj/\"\n",
        "\n",
        "train_df = pd.read_csv(data_path + \"train.csv\")\n",
        "final_test_df = pd.read_csv(data_path + \"test.csv\")\n",
        "\n",
        "train_df[\"text\"] = train_df[\"review\"]\n",
        "final_test_df[\"text\"] = final_test_df[\"review\"]\n",
        "train_df = preprocess_df(train_df)\n",
        "final_test_df = preprocess_df(final_test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label\n",
            "american (traditional)    2680\n",
            "mexican                   2217\n",
            "italian                   2032\n",
            "chinese                   1696\n",
            "american (new)            1399\n",
            "japanese                  1063\n",
            "mediterranean              728\n",
            "canadian (new)             484\n",
            "thai                       483\n",
            "asian fusion               362\n",
            "Name: count, dtype: int64\n",
            "['id', 'attributes.HappyHour', 'attributes.Ambience', 'hours.Tuesday', 'postal_code', 'attributes.AgesAllowed', 'attributes.GoodForDancing', 'attributes.OutdoorSeating', 'hours.Saturday', 'attributes.Corkage', 'longitude', 'name', 'attributes.BusinessAcceptsCreditCards', 'attributes.RestaurantsTableService', 'attributes.RestaurantsReservations', 'hours.Friday', 'attributes.RestaurantsPriceRange2', 'attributes.WiFi', 'attributes.ByAppointmentOnly', 'attributes.Music', 'attributes.NoiseLevel', 'attributes.BYOB', 'state', 'attributes.Alcohol', 'attributes.HasTV', 'attributes', 'attributes.BYOBCorkage', 'hours.Wednesday', 'hours.Sunday', 'attributes.RestaurantsGoodForGroups', 'attributes.Open24Hours', 'attributes.BusinessParking', 'attributes.DogsAllowed', 'attributes.HairSpecializesIn', 'review_count', 'is_open', 'attributes.Caters', 'attributes.CoatCheck', 'attributes.BikeParking', 'hours.Monday', 'attributes.WheelchairAccessible', 'city', 'stars', 'attributes.DriveThru', 'attributes.RestaurantsTakeOut', 'latitude', 'attributes.Smoking', 'business_id', 'attributes.RestaurantsCounterService', 'hours.Thursday', 'attributes.RestaurantsAttire', 'attributes.BestNights', 'attributes.AcceptsInsurance', 'attributes.RestaurantsDelivery', 'attributes.DietaryRestrictions', 'attributes.BusinessAcceptsBitcoin', 'address', 'attributes.GoodForKids', 'attributes.GoodForMeal', 'hours', 'label', 'review', 'text']\n",
            "Train DataFrame info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 13144 entries, 0 to 13143\n",
            "Data columns (total 63 columns):\n",
            " #   Column                                 Non-Null Count  Dtype  \n",
            "---  ------                                 --------------  -----  \n",
            " 0   id                                     13144 non-null  int64  \n",
            " 1   attributes.HappyHour                   1256 non-null   object \n",
            " 2   attributes.Ambience                    11338 non-null  object \n",
            " 3   hours.Tuesday                          10412 non-null  object \n",
            " 4   postal_code                            13144 non-null  object \n",
            " 5   attributes.AgesAllowed                 12 non-null     object \n",
            " 6   attributes.GoodForDancing              1111 non-null   object \n",
            " 7   attributes.OutdoorSeating              11845 non-null  object \n",
            " 8   hours.Saturday                         10525 non-null  object \n",
            " 9   attributes.Corkage                     182 non-null    object \n",
            " 10  longitude                              13144 non-null  float64\n",
            " 11  name                                   13144 non-null  object \n",
            " 12  attributes.BusinessAcceptsCreditCards  8814 non-null   object \n",
            " 13  attributes.RestaurantsTableService     4428 non-null   object \n",
            " 14  attributes.RestaurantsReservations     12135 non-null  object \n",
            " 15  hours.Friday                           10706 non-null  object \n",
            " 16  attributes.RestaurantsPriceRange2      12296 non-null  object \n",
            " 17  attributes.WiFi                        10222 non-null  object \n",
            " 18  attributes.ByAppointmentOnly           113 non-null    object \n",
            " 19  attributes.Music                       1224 non-null   object \n",
            " 20  attributes.NoiseLevel                  10552 non-null  object \n",
            " 21  attributes.BYOB                        5 non-null      object \n",
            " 22  state                                  13144 non-null  object \n",
            " 23  attributes.Alcohol                     11173 non-null  object \n",
            " 24  attributes.HasTV                       11268 non-null  object \n",
            " 25  attributes                             12913 non-null  object \n",
            " 26  attributes.BYOBCorkage                 443 non-null    object \n",
            " 27  hours.Wednesday                        10618 non-null  object \n",
            " 28  hours.Sunday                           9207 non-null   object \n",
            " 29  attributes.RestaurantsGoodForGroups    12325 non-null  object \n",
            " 30  attributes.Open24Hours                 1 non-null      object \n",
            " 31  attributes.BusinessParking             11559 non-null  object \n",
            " 32  attributes.DogsAllowed                 1060 non-null   object \n",
            " 33  attributes.HairSpecializesIn           0 non-null      float64\n",
            " 34  review_count                           13144 non-null  int64  \n",
            " 35  is_open                                13144 non-null  int64  \n",
            " 36  attributes.Caters                      8814 non-null   object \n",
            " 37  attributes.CoatCheck                   852 non-null    object \n",
            " 38  attributes.BikeParking                 9706 non-null   object \n",
            " 39  hours.Monday                           9763 non-null   object \n",
            " 40  attributes.WheelchairAccessible        2011 non-null   object \n",
            " 41  city                                   13144 non-null  object \n",
            " 42  stars                                  13144 non-null  float64\n",
            " 43  attributes.DriveThru                   662 non-null    object \n",
            " 44  attributes.RestaurantsTakeOut          12324 non-null  object \n",
            " 45  latitude                               13144 non-null  float64\n",
            " 46  attributes.Smoking                     774 non-null    object \n",
            " 47  business_id                            13144 non-null  object \n",
            " 48  attributes.RestaurantsCounterService   3 non-null      object \n",
            " 49  hours.Thursday                         10686 non-null  object \n",
            " 50  attributes.RestaurantsAttire           11937 non-null  object \n",
            " 51  attributes.BestNights                  907 non-null    object \n",
            " 52  attributes.AcceptsInsurance            2 non-null      object \n",
            " 53  attributes.RestaurantsDelivery         12015 non-null  object \n",
            " 54  attributes.DietaryRestrictions         1 non-null      object \n",
            " 55  attributes.BusinessAcceptsBitcoin      957 non-null    object \n",
            " 56  address                                13144 non-null  object \n",
            " 57  attributes.GoodForKids                 12248 non-null  object \n",
            " 58  attributes.GoodForMeal                 7811 non-null   object \n",
            " 59  hours                                  10743 non-null  object \n",
            " 60  label                                  13144 non-null  object \n",
            " 61  review                                 13144 non-null  object \n",
            " 62  text                                   13144 non-null  object \n",
            "dtypes: float64(4), int64(3), object(56)\n",
            "memory usage: 6.3+ MB\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>attributes.HappyHour</th>\n",
              "      <th>attributes.Ambience</th>\n",
              "      <th>hours.Tuesday</th>\n",
              "      <th>postal_code</th>\n",
              "      <th>attributes.AgesAllowed</th>\n",
              "      <th>attributes.GoodForDancing</th>\n",
              "      <th>attributes.OutdoorSeating</th>\n",
              "      <th>hours.Saturday</th>\n",
              "      <th>attributes.Corkage</th>\n",
              "      <th>...</th>\n",
              "      <th>attributes.AcceptsInsurance</th>\n",
              "      <th>attributes.RestaurantsDelivery</th>\n",
              "      <th>attributes.DietaryRestrictions</th>\n",
              "      <th>attributes.BusinessAcceptsBitcoin</th>\n",
              "      <th>address</th>\n",
              "      <th>attributes.GoodForKids</th>\n",
              "      <th>attributes.GoodForMeal</th>\n",
              "      <th>hours</th>\n",
              "      <th>review</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b\"{'touristy': False, 'hipster': False, 'roman...</td>\n",
              "      <td>b'11:30-0:0'</td>\n",
              "      <td>b'M5A 1T1'</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b'True'</td>\n",
              "      <td>b'17:0-2:0'</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b'False'</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b'366 Queen Street E'</td>\n",
              "      <td>b'False'</td>\n",
              "      <td>b\"{'dessert': False, 'latenight': False, 'lunc...</td>\n",
              "      <td>{'Tuesday': '11:30-0:0', 'Wednesday': '11:30-0...</td>\n",
              "      <td>Overall wonderful experience. \\n\\nThe owner Fa...</td>\n",
              "      <td>Overall wonderful experience nThe owner Farley...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b\"{'romantic': False, 'intimate': False, 'clas...</td>\n",
              "      <td>b'12:0-21:0'</td>\n",
              "      <td>b'M6R 2N2'</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b'True'</td>\n",
              "      <td>b'14:0-21:0'</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b'False'</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b'414 Roncesvalles Avenue'</td>\n",
              "      <td>b'True'</td>\n",
              "      <td>b\"{'dessert': False, 'latenight': False, 'lunc...</td>\n",
              "      <td>{'Monday': '12:0-21:0', 'Tuesday': '12:0-21:0'...</td>\n",
              "      <td>VIBE: Nieghbourhood hole. TV playing sports, p...</td>\n",
              "      <td>VIBE Nieghbourhood hole TV playing sports patr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b\"{'romantic': False, 'intimate': False, 'tour...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b'53704'</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b'False'</td>\n",
              "      <td>b'11:30-21:0'</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b'2817 E Washington Ave'</td>\n",
              "      <td>b'True'</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'Monday': '11:30-21:0', 'Wednesday': '11:30-2...</td>\n",
              "      <td>Viet House is very good.  The food was fresh, ...</td>\n",
              "      <td>Viet House good The food fresh well cooked fla...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b\"{'romantic': False, 'intimate': False, 'clas...</td>\n",
              "      <td>b'17:0-21:30'</td>\n",
              "      <td>b'T2E 6Z3'</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b'False'</td>\n",
              "      <td>b'17:0-22:0'</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b'False'</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b'2323 32 Avenue NE, Suite 108'</td>\n",
              "      <td>b'True'</td>\n",
              "      <td>b\"{'dessert': False, 'latenight': False, 'lunc...</td>\n",
              "      <td>{'Monday': '0:0-0:0', 'Tuesday': '17:0-21:30',...</td>\n",
              "      <td>This is a really good place. Not truly authent...</td>\n",
              "      <td>This really good place Not truly authentic Tha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>b'False'</td>\n",
              "      <td>b\"{'romantic': False, 'intimate': False, 'tour...</td>\n",
              "      <td>b'11:0-21:0'</td>\n",
              "      <td>b'85085'</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b'False'</td>\n",
              "      <td>b'True'</td>\n",
              "      <td>b'11:0-22:0'</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b'False'</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b'2470 W Happy Valley Rd'</td>\n",
              "      <td>b'True'</td>\n",
              "      <td>b\"{'dessert': False, 'latenight': False, 'lunc...</td>\n",
              "      <td>{'Monday': '0:0-0:0', 'Tuesday': '11:0-21:0', ...</td>\n",
              "      <td>This place is great. Think fast food Italian s...</td>\n",
              "      <td>This place great Think fast food Italian style...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 62 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id attributes.HappyHour                                attributes.Ambience  \\\n",
              "0   0                  NaN  b\"{'touristy': False, 'hipster': False, 'roman...   \n",
              "1   1                  NaN  b\"{'romantic': False, 'intimate': False, 'clas...   \n",
              "2   2                  NaN  b\"{'romantic': False, 'intimate': False, 'tour...   \n",
              "3   3                  NaN  b\"{'romantic': False, 'intimate': False, 'clas...   \n",
              "4   4             b'False'  b\"{'romantic': False, 'intimate': False, 'tour...   \n",
              "\n",
              "   hours.Tuesday postal_code attributes.AgesAllowed attributes.GoodForDancing  \\\n",
              "0   b'11:30-0:0'  b'M5A 1T1'                    NaN                       NaN   \n",
              "1   b'12:0-21:0'  b'M6R 2N2'                    NaN                       NaN   \n",
              "2            NaN    b'53704'                    NaN                       NaN   \n",
              "3  b'17:0-21:30'  b'T2E 6Z3'                    NaN                       NaN   \n",
              "4   b'11:0-21:0'    b'85085'                    NaN                  b'False'   \n",
              "\n",
              "  attributes.OutdoorSeating hours.Saturday attributes.Corkage  ...  \\\n",
              "0                   b'True'    b'17:0-2:0'                NaN  ...   \n",
              "1                   b'True'   b'14:0-21:0'                NaN  ...   \n",
              "2                  b'False'  b'11:30-21:0'                NaN  ...   \n",
              "3                  b'False'   b'17:0-22:0'                NaN  ...   \n",
              "4                   b'True'   b'11:0-22:0'                NaN  ...   \n",
              "\n",
              "   attributes.AcceptsInsurance attributes.RestaurantsDelivery  \\\n",
              "0                          NaN                       b'False'   \n",
              "1                          NaN                       b'False'   \n",
              "2                          NaN                            NaN   \n",
              "3                          NaN                       b'False'   \n",
              "4                          NaN                       b'False'   \n",
              "\n",
              "  attributes.DietaryRestrictions attributes.BusinessAcceptsBitcoin  \\\n",
              "0                            NaN                               NaN   \n",
              "1                            NaN                               NaN   \n",
              "2                            NaN                               NaN   \n",
              "3                            NaN                               NaN   \n",
              "4                            NaN                               NaN   \n",
              "\n",
              "                           address attributes.GoodForKids  \\\n",
              "0            b'366 Queen Street E'               b'False'   \n",
              "1       b'414 Roncesvalles Avenue'                b'True'   \n",
              "2         b'2817 E Washington Ave'                b'True'   \n",
              "3  b'2323 32 Avenue NE, Suite 108'                b'True'   \n",
              "4        b'2470 W Happy Valley Rd'                b'True'   \n",
              "\n",
              "                              attributes.GoodForMeal  \\\n",
              "0  b\"{'dessert': False, 'latenight': False, 'lunc...   \n",
              "1  b\"{'dessert': False, 'latenight': False, 'lunc...   \n",
              "2                                                NaN   \n",
              "3  b\"{'dessert': False, 'latenight': False, 'lunc...   \n",
              "4  b\"{'dessert': False, 'latenight': False, 'lunc...   \n",
              "\n",
              "                                               hours  \\\n",
              "0  {'Tuesday': '11:30-0:0', 'Wednesday': '11:30-0...   \n",
              "1  {'Monday': '12:0-21:0', 'Tuesday': '12:0-21:0'...   \n",
              "2  {'Monday': '11:30-21:0', 'Wednesday': '11:30-2...   \n",
              "3  {'Monday': '0:0-0:0', 'Tuesday': '17:0-21:30',...   \n",
              "4  {'Monday': '0:0-0:0', 'Tuesday': '11:0-21:0', ...   \n",
              "\n",
              "                                              review  \\\n",
              "0  Overall wonderful experience. \\n\\nThe owner Fa...   \n",
              "1  VIBE: Nieghbourhood hole. TV playing sports, p...   \n",
              "2  Viet House is very good.  The food was fresh, ...   \n",
              "3  This is a really good place. Not truly authent...   \n",
              "4  This place is great. Think fast food Italian s...   \n",
              "\n",
              "                                                text  \n",
              "0  Overall wonderful experience nThe owner Farley...  \n",
              "1  VIBE Nieghbourhood hole TV playing sports patr...  \n",
              "2  Viet House good The food fresh well cooked fla...  \n",
              "3  This really good place Not truly authentic Tha...  \n",
              "4  This place great Think fast food Italian style...  \n",
              "\n",
              "[5 rows x 62 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_counts = train_df['label'].value_counts()\n",
        "\n",
        "print(class_counts)\n",
        "\n",
        "print(train_df.columns.tolist())\n",
        "train_df.head()\n",
        "\n",
        "print(\"Train DataFrame info:\")\n",
        "train_df.info()\n",
        "\n",
        "final_test_df.head()\n",
        "\n",
        "# Look at categorical features (not including the target)\n",
        "# categorical_columns = X.select_dtypes(include=['object']).columns\n",
        "# print(f\"\\nCategorical columns in train data: {categorical_columns}\")\n",
        "\n",
        "# # Inspect unique values of categorical columns\n",
        "# for col in categorical_columns:\n",
        "#     print(f\"\\nUnique values in {col}:\")\n",
        "#     print(X[col].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IkpzDD4SEhU"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>review_count</th>\n",
              "      <th>is_open</th>\n",
              "      <th>stars</th>\n",
              "      <th>latitude</th>\n",
              "      <th>text</th>\n",
              "      <th>state_idx</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-79.363792</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>4.5</td>\n",
              "      <td>43.655808</td>\n",
              "      <td>Overall wonderful experience nThe owner Farley...</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-79.451277</td>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>43.651701</td>\n",
              "      <td>VIBE Nieghbourhood hole TV playing sports patr...</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-89.344794</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>43.102887</td>\n",
              "      <td>Viet House good The food fresh well cooked fla...</td>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-114.005667</td>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>51.080983</td>\n",
              "      <td>This really good place Not truly authentic Tha...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-112.113049</td>\n",
              "      <td>213</td>\n",
              "      <td>1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>33.714826</td>\n",
              "      <td>This place great Think fast food Italian style...</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    longitude  review_count  is_open  stars   latitude  \\\n",
              "0  -79.363792             7        0    4.5  43.655808   \n",
              "1  -79.451277            24        1    3.0  43.651701   \n",
              "2  -89.344794            18        0    3.5  43.102887   \n",
              "3 -114.005667            52        1    3.0  51.080983   \n",
              "4 -112.113049           213        1    3.5  33.714826   \n",
              "\n",
              "                                                text  state_idx  id  \n",
              "0  Overall wonderful experience nThe owner Farley...          8   0  \n",
              "1  VIBE Nieghbourhood hole TV playing sports patr...          8   1  \n",
              "2  Viet House good The food fresh well cooked fla...         13   2  \n",
              "3  This really good place Not truly authentic Tha...          0   3  \n",
              "4  This place great Think fast food Italian style...          1   4  "
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = train_df.drop(columns=[\"label\"])\n",
        "y = train_df['label']\n",
        "X = X.dropna(axis=1, thresh=13144)\n",
        "X = X.drop(columns=[\"business_id\", \"address\", \"id\", \"city\", \"postal_code\", \"name\", \"review\"])\n",
        "\n",
        "all_states = sorted(set(train_df['state'].unique()).union(set(final_test_df['state'].unique())))\n",
        "state_to_idx = {state: i for i, state in enumerate(all_states)}\n",
        "unknown_state_idx = len(state_to_idx)\n",
        "\n",
        "def map_state_column(series, mapping, unk_idx):\n",
        "    return series.apply(lambda x: mapping.get(x.decode() if isinstance(x, bytes) else x, unk_idx))\n",
        "\n",
        "le = LabelEncoder()\n",
        "X['state_idx'] = map_state_column(X['state'], state_to_idx, unknown_state_idx)\n",
        "final_test_df['state_idx'] = map_state_column(final_test_df['state'], state_to_idx, unknown_state_idx)\n",
        "\n",
        "X = X.drop(columns=[\"state\"])\n",
        "\n",
        "label_le = LabelEncoder()\n",
        "y_encoded = label_le.fit_transform(y)\n",
        "label_mapping = dict(zip(label_le.transform(label_le.classes_), label_le.classes_))\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size = 0.1, random_state=42)\n",
        "\n",
        "final_test_df = final_test_df[X.columns.intersection(final_test_df.columns).tolist() + ['id']]\n",
        "final_test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13\n",
            "13\n"
          ]
        }
      ],
      "source": [
        "print(X['state_idx'].max())\n",
        "print(final_test_df['state_idx'].max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # tokenization\n",
        "# tagged_data = [word_tokenize(_d) for i, _d in enumerate(train_df[\"text\"])]\n",
        "# # build vocabulary from tokenized data\n",
        "# word_counts, vocabulary, vocabulary_inv = build_vocab(tagged_data)\n",
        "# # use the above mapping to create input data\n",
        "# inp_data = [[vocabulary[word] for word in text] for text in tagged_data]\n",
        "# # get embedding vector\n",
        "# embedding_weights = get_embeddings(inp_data, vocabulary_inv)\n",
        "\n",
        "\n",
        "# tagged_train_data = [word_tokenize(_d) for i, _d in enumerate(train_df[\"text\"])]\n",
        "# tagged_test_data = [word_tokenize(_d) for i, _d in enumerate(test_df[\"text\"])]\n",
        "\n",
        "# embedding_dim = embedding_weights.shape[1]\n",
        "# vocab_size = embedding_weights.shape[0]\n",
        "\n",
        "# print(embedding_weights)\n",
        "# print(tagged_train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hloBnIr7McIy",
        "outputId": "f9059219-e349-48fb-c77c-fc568a6aa2a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Word2Vec model...\n",
            "Model: skip-gram\n",
            "Saving Word2Vec model embedding\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\danny\\AppData\\Local\\Temp\\ipykernel_20360\\3255774520.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.text_seqs = torch.tensor(text_seqs, dtype=torch.long)\n",
            "C:\\Users\\danny\\AppData\\Local\\Temp\\ipykernel_20360\\3255774520.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.state_idxs = torch.tensor(state_idxs, dtype=torch.long)\n",
            "C:\\Users\\danny\\AppData\\Local\\Temp\\ipykernel_20360\\3255774520.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.structured_feats = torch.tensor(structured_feats, dtype=torch.float32)\n",
            "C:\\Users\\danny\\AppData\\Local\\Temp\\ipykernel_20360\\3255774520.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.labels = torch.tensor(labels, dtype=torch.long)\n"
          ]
        }
      ],
      "source": [
        "tagged_train_data = [word_tokenize(text) for text in X_train[\"text\"]]\n",
        "tagged_val_data = [word_tokenize(text) for text in X_val[\"text\"]]\n",
        "tagged_test_data = [word_tokenize(text) for text in final_test_df[\"text\"]]\n",
        "\n",
        "word_counts, vocabulary, vocabulary_inv = build_vocab(tagged_train_data)\n",
        "\n",
        "inp_data = [[vocabulary[word] for word in text] for text in tagged_train_data]\n",
        "\n",
        "embedding_weights = get_embeddings(inp_data, vocabulary_inv, size_features=100, mode='skipgram')\n",
        "vocab_size, embed_dim = embedding_weights.shape\n",
        "\n",
        "train_seq = [[vocabulary[word] for word in text if word in vocabulary] for text in tagged_train_data]\n",
        "val_seq = [[vocabulary[word] for word in text if word in vocabulary] for text in tagged_val_data]\n",
        "test_seq = [[vocabulary[word] for word in text if word in vocabulary] for text in tagged_test_data]\n",
        "\n",
        "train_text_seqs = pad_sequence([torch.tensor(x) for x in train_seq], batch_first=True, padding_value=0)\n",
        "val_text_seqs = pad_sequence([torch.tensor(x) for x in val_seq], batch_first=True, padding_value=0)\n",
        "test_text_seqs = pad_sequence([torch.tensor(x) for x in test_seq], batch_first=True, padding_value=0)\n",
        "\n",
        "structured_cols = [\"longitude\", \"latitude\", \"review_count\", \"is_open\", \"stars\"]\n",
        "train_feats = torch.tensor(X_train[structured_cols].values, dtype=torch.float32)\n",
        "val_feats = torch.tensor(X_val[structured_cols].values, dtype=torch.float32)\n",
        "structured_feats_test = torch.tensor(final_test_df[structured_cols].values, dtype=torch.float32)\n",
        "\n",
        "train_state_idxs = torch.tensor(X_train[\"state_idx\"].values, dtype=torch.long)\n",
        "val_state_idxs = torch.tensor(X_val[\"state_idx\"].values, dtype=torch.long)\n",
        "test_state_idxs = torch.tensor(final_test_df[\"state_idx\"].values, dtype=torch.long)\n",
        "\n",
        "train_labels = torch.tensor(y_train, dtype=torch.long)\n",
        "val_labels = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "dummy_labels = torch.zeros(len(final_test_df), dtype=torch.long)\n",
        "\n",
        "train_dataset = RestaurantDataset(train_text_seqs, train_state_idxs, train_feats, train_labels)\n",
        "val_dataset = RestaurantDataset(val_text_seqs, val_state_idxs, val_feats, val_labels)\n",
        "test_dataset = RestaurantDataset(\n",
        "    text_seqs=test_text_seqs,\n",
        "    state_idxs=test_state_idxs,\n",
        "    structured_feats=structured_feats_test,\n",
        "    labels=dummy_labels,\n",
        "    id=torch.tensor(final_test_df[\"id\"].values)  # dummy labels, if needed\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max state_idx in train_loader: 13\n",
            "state_vocab_size: 15\n"
          ]
        }
      ],
      "source": [
        "print(\"Max state_idx in train_loader:\", max([s.max().item() for _, s, _, _ in train_loader]))\n",
        "print(\"state_vocab_size:\", len(state_to_idx) + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKULWt_FMYbr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Loss: 4.6568 - Val Acc: 0.2084\n",
            "Epoch 2 - Loss: 2.0365 - Val Acc: 0.2327\n",
            "Epoch 3 - Loss: 2.0130 - Val Acc: 0.2304\n",
            "Epoch 4 - Loss: 1.9967 - Val Acc: 0.2357\n",
            "Epoch 5 - Loss: 1.9857 - Val Acc: 0.2471\n",
            "Epoch 6 - Loss: 1.9854 - Val Acc: 0.2441\n",
            "Epoch 7 - Loss: 1.9770 - Val Acc: 0.2335\n",
            "Epoch 8 - Loss: 1.9835 - Val Acc: 0.2418\n",
            "Epoch 9 - Loss: 1.9763 - Val Acc: 0.2289\n",
            "Epoch 10 - Loss: 1.9775 - Val Acc: 0.2373\n",
            "Epoch 11 - Loss: 1.9765 - Val Acc: 0.2373\n",
            "Epoch 12 - Loss: 1.9772 - Val Acc: 0.2418\n",
            "Epoch 13 - Loss: 1.9702 - Val Acc: 0.2578\n",
            "Epoch 14 - Loss: 1.9752 - Val Acc: 0.2289\n",
            "Epoch 15 - Loss: 1.9682 - Val Acc: 0.2426\n",
            "Epoch 16 - Loss: 1.9696 - Val Acc: 0.2433\n",
            "Epoch 17 - Loss: 1.9688 - Val Acc: 0.2464\n",
            "Epoch 18 - Loss: 1.9731 - Val Acc: 0.2418\n",
            "Epoch 19 - Loss: 1.9642 - Val Acc: 0.2433\n",
            "Epoch 20 - Loss: 1.9686 - Val Acc: 0.2251\n",
            "Epoch 21 - Loss: 1.9719 - Val Acc: 0.2418\n",
            "Epoch 22 - Loss: 1.9609 - Val Acc: 0.2403\n",
            "Epoch 23 - Loss: 1.9608 - Val Acc: 0.2403\n",
            "Epoch 24 - Loss: 1.9633 - Val Acc: 0.2335\n",
            "Epoch 25 - Loss: 1.9724 - Val Acc: 0.2274\n",
            "Epoch 26 - Loss: 1.9658 - Val Acc: 0.2456\n",
            "Epoch 27 - Loss: 1.9690 - Val Acc: 0.2578\n",
            "Epoch 28 - Loss: 1.9647 - Val Acc: 0.2281\n",
            "Epoch 29 - Loss: 1.9670 - Val Acc: 0.2304\n",
            "Epoch 30 - Loss: 1.9661 - Val Acc: 0.2456\n",
            "Epoch 31 - Loss: 1.9632 - Val Acc: 0.2471\n",
            "Epoch 32 - Loss: 1.9619 - Val Acc: 0.2274\n",
            "Epoch 33 - Loss: 1.9811 - Val Acc: 0.2327\n",
            "Epoch 34 - Loss: 1.9655 - Val Acc: 0.2395\n",
            "Epoch 35 - Loss: 1.9575 - Val Acc: 0.2555\n",
            "Epoch 36 - Loss: 1.6417 - Val Acc: 0.3673\n",
            "Epoch 37 - Loss: 1.3857 - Val Acc: 0.4365\n",
            "Epoch 38 - Loss: 1.0548 - Val Acc: 0.6966\n",
            "Epoch 39 - Loss: 0.7023 - Val Acc: 0.7034\n",
            "Epoch 40 - Loss: 0.5233 - Val Acc: 0.7574\n",
            "Epoch 41 - Loss: 0.3680 - Val Acc: 0.7483\n",
            "Epoch 42 - Loss: 0.2386 - Val Acc: 0.7468\n",
            "Epoch 43 - Loss: 0.1409 - Val Acc: 0.7354\n",
            "Epoch 44 - Loss: 0.1104 - Val Acc: 0.7308\n",
            "Epoch 45 - Loss: 0.0694 - Val Acc: 0.7369\n",
            "Epoch 46 - Loss: 0.0512 - Val Acc: 0.7240\n",
            "Epoch 47 - Loss: 0.0351 - Val Acc: 0.7376\n",
            "Epoch 48 - Loss: 0.0288 - Val Acc: 0.7179\n",
            "Epoch 49 - Loss: 0.0291 - Val Acc: 0.7125\n",
            "Epoch 50 - Loss: 0.0257 - Val Acc: 0.7247\n",
            "Epoch 51 - Loss: 0.0210 - Val Acc: 0.7217\n",
            "Epoch 52 - Loss: 0.0142 - Val Acc: 0.7247\n",
            "Epoch 53 - Loss: 0.0134 - Val Acc: 0.7255\n",
            "Epoch 54 - Loss: 0.0100 - Val Acc: 0.7217\n",
            "Epoch 55 - Loss: 0.0290 - Val Acc: 0.7072\n",
            "Epoch 56 - Loss: 0.0212 - Val Acc: 0.6973\n",
            "Epoch 57 - Loss: 0.0105 - Val Acc: 0.7103\n",
            "Epoch 58 - Loss: 0.0147 - Val Acc: 0.7049\n",
            "Epoch 59 - Loss: 0.0052 - Val Acc: 0.7118\n",
            "Epoch 60 - Loss: 0.0027 - Val Acc: 0.7141\n",
            "Epoch 61 - Loss: 0.0017 - Val Acc: 0.7163\n",
            "Epoch 62 - Loss: 0.0014 - Val Acc: 0.7141\n",
            "Epoch 63 - Loss: 0.0012 - Val Acc: 0.7209\n",
            "Epoch 64 - Loss: 0.0010 - Val Acc: 0.7156\n",
            "Epoch 65 - Loss: 0.0010 - Val Acc: 0.7156\n",
            "Epoch 66 - Loss: 0.0008 - Val Acc: 0.7148\n",
            "Epoch 67 - Loss: 0.0007 - Val Acc: 0.7133\n",
            "Epoch 68 - Loss: 0.0006 - Val Acc: 0.7133\n",
            "Epoch 69 - Loss: 0.0005 - Val Acc: 0.7110\n",
            "Epoch 70 - Loss: 0.0005 - Val Acc: 0.7103\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Model setup\n",
        "early_stopping = EarlyStopping(patience=10, min_delta=0.01)\n",
        "num_epochs = 10000\n",
        "\n",
        "model = RestaurantRNN(\n",
        "    vocab_size=vocab_size,\n",
        "    state_vocab_size=len(state_to_idx) + 1,\n",
        "    embed_dim=embed_dim,\n",
        "    state_embed_dim=8,\n",
        "    hidden_dim=128,\n",
        "    num_structured_feats=train_feats.shape[1],\n",
        "    num_classes=len(set(train_labels))\n",
        ")\n",
        "model.word_embedding.weight.data.copy_(torch.tensor(embedding_weights))\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "#     optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        "# )\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for text, state_idx, feats, label in train_loader:\n",
        "        text, state_idx, feats, label = text.to(device), state_idx.to(device), feats.to(device), label.to(device)\n",
        "\n",
        "        output = model(text, state_idx, feats)\n",
        "        loss = criterion(output, label)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for text, state_idx, feats, label in val_loader:\n",
        "            text, state_idx, feats, label = text.to(device), state_idx.to(device), feats.to(device), label.to(device)\n",
        "            preds = model(text, state_idx, feats)\n",
        "            loss = criterion(preds, label)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            pred_classes = preds.argmax(dim=1)\n",
        "            correct += (pred_classes == label).sum().item()\n",
        "            total += label.size(0)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_acc = correct / total\n",
        "\n",
        "    print(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f} - Val Acc: {correct / total:.4f}\")\n",
        "\n",
        "    early_stopping(avg_val_loss, model)\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break\n",
        "\n",
        "# After training ends, load the best model state:\n",
        "model.load_state_dict(early_stopping.best_model_state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Word2Vec model...\n",
            "Model: skip-gram\n",
            "Saving Word2Vec model embedding\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\danny\\AppData\\Local\\Temp\\ipykernel_20360\\3255774520.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.text_seqs = torch.tensor(text_seqs, dtype=torch.long)\n",
            "C:\\Users\\danny\\AppData\\Local\\Temp\\ipykernel_20360\\3255774520.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.state_idxs = torch.tensor(state_idxs, dtype=torch.long)\n",
            "C:\\Users\\danny\\AppData\\Local\\Temp\\ipykernel_20360\\3255774520.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.structured_feats = torch.tensor(structured_feats, dtype=torch.float32)\n",
            "C:\\Users\\danny\\AppData\\Local\\Temp\\ipykernel_20360\\3255774520.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.labels = torch.tensor(labels, dtype=torch.long)\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "full_df = pd.concat([X_train, X_val], ignore_index=True)\n",
        "full_labels = np.concatenate([y_train, y_val])\n",
        "\n",
        "tagged_full_data = [word_tokenize(text) for text in full_df[\"text\"]]\n",
        "\n",
        "word_counts_full, vocabulary_full, vocabulary_inv_full = build_vocab(tagged_full_data)\n",
        "\n",
        "full_seq = [[vocabulary_full[word] for word in text if word in vocabulary_full] for text in tagged_full_data]\n",
        "\n",
        "# Pad sequences\n",
        "full_text_seqs = pad_sequence([torch.tensor(x) for x in full_seq], batch_first=True, padding_value=0)\n",
        "\n",
        "structured_cols = [\"longitude\", \"latitude\", \"review_count\", \"is_open\", \"stars\"]\n",
        "full_feats = torch.tensor(full_df[structured_cols].values, dtype=torch.float32)\n",
        "\n",
        "full_state_idxs = torch.tensor(full_df[\"state_idx\"].values, dtype=torch.long)\n",
        "\n",
        "# 7. Prepare labels tensor for full dataset\n",
        "full_labels_tensor = torch.tensor(full_labels, dtype=torch.long)\n",
        "\n",
        "# --- New: Get embedding weights for full vocab ---\n",
        "embedding_weights_full = get_embeddings(full_seq, vocabulary_inv_full, size_features=100, mode='skipgram')\n",
        "#embedding_weights_full = torch.tensor(embedding_weights_full, dtype=torch.float32)\n",
        "vocab_size, embed_dim = embedding_weights_full.shape  # Update vocab size and embedding dim accordingly\n",
        "\n",
        "# 8. Create full dataset and dataloader\n",
        "full_dataset = RestaurantDataset(full_text_seqs, full_state_idxs, full_feats, full_labels_tensor)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    texts, states, feats, labels = zip(*batch)\n",
        "    texts_padded = pad_sequence(texts, batch_first=True, padding_value=0)\n",
        "    states = torch.stack(states)\n",
        "    feats = torch.stack(feats)\n",
        "    labels = torch.stack(labels)\n",
        "    return texts_padded, states, feats, labels\n",
        "\n",
        "full_train_loader = DataLoader(full_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Full training epoch 1, Loss: 2.9138\n",
            "Full training epoch 2, Loss: 2.0647\n",
            "Full training epoch 3, Loss: 1.9642\n",
            "Full training epoch 4, Loss: 1.3344\n",
            "Full training epoch 5, Loss: 0.9103\n",
            "Full training epoch 6, Loss: 0.6856\n",
            "Full training epoch 7, Loss: 0.5251\n",
            "Full training epoch 8, Loss: 0.4019\n",
            "Full training epoch 9, Loss: 0.2923\n",
            "Full training epoch 10, Loss: 0.1943\n",
            "Full training epoch 11, Loss: 0.1269\n",
            "Full training epoch 12, Loss: 0.0875\n",
            "Full training epoch 13, Loss: 0.0626\n",
            "Full training epoch 14, Loss: 0.0525\n",
            "Full training epoch 15, Loss: 0.0449\n",
            "Full training epoch 16, Loss: 0.0357\n",
            "Full training epoch 17, Loss: 0.0250\n",
            "Full training epoch 18, Loss: 0.0223\n",
            "Full training epoch 19, Loss: 0.0233\n",
            "Full training epoch 20, Loss: 0.0287\n",
            "Full training epoch 21, Loss: 0.0203\n",
            "Full training epoch 22, Loss: 0.0173\n",
            "Full training epoch 23, Loss: 0.0174\n",
            "Full training epoch 24, Loss: 0.0111\n",
            "Full training epoch 25, Loss: 0.0203\n",
            "Full training epoch 26, Loss: 0.0136\n",
            "Full training epoch 27, Loss: 0.0071\n",
            "Full training epoch 28, Loss: 0.0037\n",
            "Full training epoch 29, Loss: 0.0035\n",
            "Full training epoch 30, Loss: 0.0025\n",
            "Full training epoch 31, Loss: 0.0016\n",
            "Full training epoch 32, Loss: 0.0019\n",
            "Full training epoch 33, Loss: 0.0016\n",
            "Full training epoch 34, Loss: 0.0022\n",
            "Early stopping triggered\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "num_classes = len(set(full_labels))  # should be same as model num_classes\n",
        "vocab_size_full, embed_dim_full = embedding_weights_full.shape\n",
        "early_stopping = EarlyStopping(patience=10, min_delta=0.01)\n",
        "num_epochs_full = 10000\n",
        "#num_states = len(le.classes_) + 1\n",
        "\n",
        "model_full = RestaurantRNN(\n",
        "    vocab_size=vocab_size_full,\n",
        "    state_vocab_size=len(state_to_idx) + 1,\n",
        "    embed_dim=embed_dim_full,\n",
        "    state_embed_dim=8,\n",
        "    hidden_dim=128,\n",
        "    num_structured_feats=train_feats.shape[1],\n",
        "    num_classes=num_classes\n",
        ")\n",
        "model_full.word_embedding.weight.data.copy_(torch.tensor(embedding_weights_full))\n",
        "model_full.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model_full.parameters(), lr=1e-3)\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "#     optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        "# )\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(num_epochs_full):\n",
        "    model_full.train()\n",
        "    total_loss = 0\n",
        "    for text, state_idx, feats, label in full_train_loader:\n",
        "        text, state_idx, feats, label = text.to(device), state_idx.to(device), feats.to(device), label.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model_full(text, state_idx, feats)\n",
        "        loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    avg_loss = total_loss / len(full_train_loader)\n",
        "    print(f\"Full training epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Call early stopping\n",
        "    early_stopping(avg_loss, model_full)\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prediction function\n",
        "def save_predictions(model, dataloader, device, output_path, label_mapping, id_series):\n",
        "\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for text, state_idx, feats, _ in dataloader:  # Ignore dummy label\n",
        "            text, state_idx, feats = text.to(device), state_idx.to(device), feats.to(device)\n",
        "            outputs = model(text, state_idx, feats)\n",
        "            predicted = torch.argmax(outputs, dim=1)\n",
        "            preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    pred_labels = [label_mapping[p] for p in preds]\n",
        "    df_out = pd.DataFrame({'Id': id_series.values, 'label': pred_labels})\n",
        "    df_out.to_csv(output_path, index=False)\n",
        "\n",
        "# Save predictions to file\n",
        "output_path = \"/Users/danny/OneDrive/Documents/UCSD/DSC 258R/kaggle_proj/rnn_preds/predicted.csv\"\n",
        "save_predictions(model_full, test_loader, device, output_path, label_mapping, final_test_df[\"id\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def save_predictions(model, dataloader, device, output_path):\n",
        "#     model.eval()\n",
        "#     preds = []\n",
        "#     with torch.no_grad():\n",
        "#         for text, state_idx, feats in dataloader:  # <-- removed the trailing comma and _\n",
        "#             text, state_idx, feats = text.to(device), state_idx.to(device), feats.to(device)\n",
        "#             outputs = model(text, state_idx, feats)\n",
        "#             predicted = torch.argmax(outputs, dim=1)\n",
        "#             preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "#     df_out = pd.DataFrame({'label': preds})\n",
        "#     df_out.to_csv(output_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# in your implementation, create the output file using the same format\n",
        "dic = {\"Id\": [], \"Predicted\": []}\n",
        "for i, pred in enumerate(preds):\n",
        "    dic[\"Id\"].append(i)\n",
        "    dic[\"Predicted\"].append(pred)\n",
        "\n",
        "dic_df = pd.DataFrame.from_dict(dic)\n",
        "dic_df.to_csv(data_path + \"predicted.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "Qtivdf7XCxdE",
        "outputId": "aa6d0b00-5678-48b1-a69e-dda4865ef4cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Word2Vec model...\n",
            "Model: skip-gram\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-249c14f46fc9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0minp_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# get embedding vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0membedding_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary_inv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-9533df6db12e>\u001b[0m in \u001b[0;36mget_embeddings\u001b[0;34m(inp_data, vocabulary_inv, size_features, mode, min_word_count, context)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model: CBOW'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,\n\u001b[0m\u001b[1;32m     20\u001b[0m                                         \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                                         \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             self.train(\n\u001b[0m\u001b[1;32m    431\u001b[0m                 \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcorpus_iterable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[0m\u001b[1;32m   1074\u001b[0m                     \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m                     \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[1;32m   1432\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[0m\u001b[1;32m   1435\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_corpus_file_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#baseline\n",
        "train_vec = []\n",
        "for doc in tagged_train_data:\n",
        "    vec = 0\n",
        "    for w in doc:\n",
        "        vec += embedding_weights[vocabulary[w]]\n",
        "    vec = vec / len(doc)\n",
        "    train_vec.append(vec)\n",
        "\n",
        "test_vec = []\n",
        "for doc in tagged_test_data:\n",
        "    vec = 0\n",
        "    length = 0\n",
        "    for w in doc:\n",
        "        try:\n",
        "            vec += embedding_weights[vocabulary[w]]\n",
        "            length += 1\n",
        "        except:\n",
        "            continue\n",
        "    vec = vec / length\n",
        "    test_vec.append(vec)\n",
        "\n",
        "clf = LogisticRegression(max_iter=100000000).fit(train_vec, df_train[\"label\"])\n",
        "preds = clf.predict(test_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naetslnoDTqf"
      },
      "outputs": [],
      "source": [
        "# in your implementation, create the output file using the same format\n",
        "dic = {\"Id\": [], \"Predicted\": []}\n",
        "for i, pred in enumerate(preds):\n",
        "    dic[\"Id\"].append(i)\n",
        "    dic[\"Predicted\"].append(pred)\n",
        "\n",
        "dic_df = pd.DataFrame.from_dict(dic)\n",
        "dic_df.to_csv(data_path + \"predicted.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rnn_model_258",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
