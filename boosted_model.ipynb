{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7c8732a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\danny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\danny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import word2vec\n",
    "from xgboost import XGBClassifier\n",
    "from joblib import Memory\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import os\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "062b52fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEmbeddingVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary, embedding_weights, size_features=100):\n",
    "        self.vocabulary = vocabulary\n",
    "        self.embedding_weights = embedding_weights\n",
    "        self.size_features = size_features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        features = []\n",
    "        for sent in X:\n",
    "            vecs = [self.embedding_weights[self.vocabulary[word]]\n",
    "                    for word in sent if word in self.vocabulary]\n",
    "            if vecs:\n",
    "                features.append(np.mean(vecs, axis=0))\n",
    "            else:\n",
    "                features.append(np.zeros(self.size_features))\n",
    "        return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c38da67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(inp_data, vocabulary_inv, size_features=100,\n",
    "                   mode='skipgram',\n",
    "                   min_word_count=2,\n",
    "                   context=5):\n",
    "    model_name = \"embedding\"\n",
    "    model_name = os.path.join(model_name)\n",
    "    num_workers = 15\n",
    "    downsampling = 1e-3 \n",
    "    print('Training Word2Vec model...')\n",
    "    # use inp_data and vocabulary_inv to reconstruct sentences\n",
    "    sentences = [[vocabulary_inv[w] for w in s] for s in inp_data]\n",
    "    if mode == 'skipgram':\n",
    "        sg = 1\n",
    "        print('Model: skip-gram')\n",
    "    elif mode == 'cbow':\n",
    "        sg = 0\n",
    "        print('Model: CBOW')\n",
    "    embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,\n",
    "                                        sg=sg,\n",
    "                                        vector_size=size_features,\n",
    "                                        min_count=min_word_count,\n",
    "                                        window=context,\n",
    "                                        sample=downsampling)\n",
    "    print(\"Saving Word2Vec model {}\".format(model_name))\n",
    "    embedding_weights = np.zeros((len(vocabulary_inv), size_features))\n",
    "    for i in range(len(vocabulary_inv)):\n",
    "        word = vocabulary_inv[i]\n",
    "        if word in embedding_model.wv:\n",
    "            embedding_weights[i] = embedding_model.wv[word]\n",
    "        else:\n",
    "            embedding_weights[i] = np.random.uniform(-0.25, 0.25,\n",
    "                                                     embedding_model.vector_size)\n",
    "    return embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ba6f77ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences):\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return word_counts, vocabulary, vocabulary_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c3fde309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_bytes(val):\n",
    "    if isinstance(val, bytes):\n",
    "        try:\n",
    "            return val.decode('utf-8').strip(\"ub'\\\"\")  # remove u'', b'', extra quotes\n",
    "        except:\n",
    "            return val\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "09d58fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    # get English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('would')\n",
    "    # prepare translation table to translate punctuation to space\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    preprocessed_sentences = []\n",
    "    for i, row in df.iterrows():\n",
    "        sent = row[\"text\"]\n",
    "        sent_nopuncts = sent.translate(translator)\n",
    "        words_list = sent_nopuncts.strip().split()\n",
    "        filtered_words = [word for word in words_list if word not in stop_words and len(word) != 1]\n",
    "        preprocessed_sentences.append(\" \".join(filtered_words))\n",
    "    df[\"text\"] = preprocessed_sentences\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ff3db597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n",
      "Model: skip-gram\n",
      "Saving Word2Vec model embedding\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/Users/danny/OneDrive/Documents/UCSD/DSC 258R/kaggle_proj/\"\n",
    "\n",
    "# 1. Load training dataset (with labels)\n",
    "train_df = pd.read_csv(data_path + \"train.csv\")\n",
    "test_df = pd.read_csv(data_path + \"test.csv\")\n",
    "train_df[\"text\"] = train_df[\"review\"]\n",
    "test_df[\"text\"] = test_df[\"review\"]\n",
    "\n",
    "train_df = preprocess_df(train_df)  # Applies stopword removal & punctuation cleanup\n",
    "test_df = preprocess_df(test_df)\n",
    "\n",
    "tagged_train_data = [word_tokenize(sent) for sent in train_df[\"text\"]]\n",
    "tagged_test_data = [word_tokenize(sent) for sent in test_df[\"text\"]]\n",
    "\n",
    "train_df[\"tokens\"] = tagged_train_data\n",
    "test_df[\"tokens\"] = tagged_test_data\n",
    "\n",
    "word_counts, vocabulary, vocabulary_inv = build_vocab(tagged_train_data)\n",
    "inp_data = [[vocabulary[word] for word in sent if word in vocabulary] for sent in tagged_train_data]\n",
    "embedding_weights = get_embeddings(inp_data, vocabulary_inv, size_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e0b68380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded with 13144 rows and 23 columns\n",
      "Test data loaded with 10000 rows and 49 columns\n",
      "\n",
      "Target column: label\n",
      "Target value distribution:\n",
      "label\n",
      "american (traditional)    2680\n",
      "mexican                   2217\n",
      "italian                   2032\n",
      "chinese                   1696\n",
      "american (new)            1399\n",
      "japanese                  1063\n",
      "mediterranean              728\n",
      "canadian (new)             484\n",
      "thai                       483\n",
      "asian fusion               362\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "american (traditional)    2680\n",
      "mexican                   2217\n",
      "italian                   2032\n",
      "chinese                   1696\n",
      "american (new)            1399\n",
      "japanese                  1063\n",
      "mediterranean              728\n",
      "canadian (new)             484\n",
      "thai                       483\n",
      "asian fusion               362\n",
      "Name: count, dtype: int64\n",
      "['attributes.Ambience', 'attributes.OutdoorSeating', 'longitude', 'name', 'attributes.RestaurantsReservations', 'attributes.RestaurantsPriceRange2', 'attributes.NoiseLevel', 'state', 'attributes.Alcohol', 'attributes.HasTV', 'attributes.RestaurantsGoodForGroups', 'attributes.BusinessParking', 'review_count', 'is_open', 'city', 'stars', 'attributes.RestaurantsTakeOut', 'latitude', 'attributes.RestaurantsAttire', 'attributes.RestaurantsDelivery', 'attributes.GoodForKids', 'label', 'tokens']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danny\\AppData\\Local\\Temp\\ipykernel_21120\\2964644820.py:11: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  train_df = train_df.applymap(decode_bytes)\n",
      "C:\\Users\\danny\\AppData\\Local\\Temp\\ipykernel_21120\\2964644820.py:12: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  test_df = test_df.applymap(decode_bytes)\n"
     ]
    }
   ],
   "source": [
    "valid_columns = train_df.columns[train_df.count() >= 10000]\n",
    "\n",
    "# Filter all datasets to keep only those columns\n",
    "train_df = train_df[valid_columns]\n",
    "\n",
    "cols_to_drop = ['id', 'hours.Tuesday', 'hours.Saturday', 'hours.Friday', 'attributes.WiFi', 'postal_code', 'hours.Thursday', 'text', 'review', 'address', 'hours.Wednesday', 'hours', 'business_id', 'attributes']\n",
    "\n",
    "train_df = train_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "test_df = test_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "train_df = train_df.applymap(decode_bytes)\n",
    "test_df = test_df.applymap(decode_bytes)\n",
    "\n",
    "non_numeric_cols = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "numeric_cols = train_df.select_dtypes(include=[np.number]).columns.difference(['label']).tolist()\n",
    "\n",
    "cols_to_exclude = ['label', 'tokens']  # add more if needed\n",
    "categorical_cols = [col for col in non_numeric_cols if col not in cols_to_exclude]\n",
    "\n",
    "print(f\"Training data loaded with {train_df.shape[0]} rows and {train_df.shape[1]} columns\")\n",
    "print(f\"Test data loaded with {test_df.shape[0]} rows and {test_df.shape[1]} columns\")\n",
    "\n",
    "# 3. Verify the training data has the label column\n",
    "target_column = 'label'\n",
    "\n",
    "print(f\"\\nTarget column: {target_column}\")\n",
    "print(\"Target value distribution:\")\n",
    "print(train_df[target_column].value_counts())\n",
    "\n",
    "class_counts = train_df['label'].value_counts()\n",
    "\n",
    "print(class_counts)\n",
    "\n",
    "print(train_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "60baeab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attributes.Ambience</th>\n",
       "      <th>attributes.OutdoorSeating</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>attributes.RestaurantsReservations</th>\n",
       "      <th>attributes.RestaurantsPriceRange2</th>\n",
       "      <th>attributes.NoiseLevel</th>\n",
       "      <th>state</th>\n",
       "      <th>attributes.Alcohol</th>\n",
       "      <th>attributes.HasTV</th>\n",
       "      <th>...</th>\n",
       "      <th>is_open</th>\n",
       "      <th>city</th>\n",
       "      <th>stars</th>\n",
       "      <th>attributes.RestaurantsTakeOut</th>\n",
       "      <th>latitude</th>\n",
       "      <th>attributes.RestaurantsAttire</th>\n",
       "      <th>attributes.RestaurantsDelivery</th>\n",
       "      <th>attributes.GoodForKids</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b\"{'romantic': False, 'intimate': False, 'clas...</td>\n",
       "      <td>b'False'</td>\n",
       "      <td>-81.820568</td>\n",
       "      <td>b'Rush Inn'</td>\n",
       "      <td>b'False'</td>\n",
       "      <td>b'2'</td>\n",
       "      <td>b\"u'loud'\"</td>\n",
       "      <td>b'OH'</td>\n",
       "      <td>b\"u'full_bar'\"</td>\n",
       "      <td>b'True'</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>b'Lakewood'</td>\n",
       "      <td>4.0</td>\n",
       "      <td>b'True'</td>\n",
       "      <td>41.484197</td>\n",
       "      <td>b\"u'casual'\"</td>\n",
       "      <td>b'False'</td>\n",
       "      <td>b'False'</td>\n",
       "      <td>american (traditional)</td>\n",
       "      <td>[So, stopped, way, Side, Quest, street, nWe, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b\"{'romantic': False, 'intimate': False, 'tour...</td>\n",
       "      <td>b'True'</td>\n",
       "      <td>-112.032893</td>\n",
       "      <td>b'GreenMix'</td>\n",
       "      <td>b'False'</td>\n",
       "      <td>b'2'</td>\n",
       "      <td>b\"u'quiet'\"</td>\n",
       "      <td>b'AZ'</td>\n",
       "      <td>b\"u'none'\"</td>\n",
       "      <td>b'False'</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>b'Phoenix'</td>\n",
       "      <td>3.5</td>\n",
       "      <td>b'True'</td>\n",
       "      <td>33.379283</td>\n",
       "      <td>b\"u'casual'\"</td>\n",
       "      <td>b'False'</td>\n",
       "      <td>b'True'</td>\n",
       "      <td>american (new)</td>\n",
       "      <td>[This, go, healthy, spot, The, food, always, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-79.339163</td>\n",
       "      <td>b'BarBurrito - Gerrard'</td>\n",
       "      <td>b'False'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'ON'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>b'Toronto'</td>\n",
       "      <td>3.0</td>\n",
       "      <td>b'False'</td>\n",
       "      <td>43.669144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mexican</td>\n",
       "      <td>[Food, court, meal, Gerrard, Square, It, since...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b\"{'romantic': False, 'intimate': False, 'clas...</td>\n",
       "      <td>b'False'</td>\n",
       "      <td>-115.242714</td>\n",
       "      <td>b'SalvaMex'</td>\n",
       "      <td>b'False'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b\"'quiet'\"</td>\n",
       "      <td>b'NV'</td>\n",
       "      <td>b\"'none'\"</td>\n",
       "      <td>b'True'</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>b'Las Vegas'</td>\n",
       "      <td>4.0</td>\n",
       "      <td>b'True'</td>\n",
       "      <td>36.159527</td>\n",
       "      <td>b\"'casual'\"</td>\n",
       "      <td>b'True'</td>\n",
       "      <td>b'True'</td>\n",
       "      <td>mexican</td>\n",
       "      <td>[Located, Rainbow, Charleston, small, family, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b\"{'romantic': False, 'intimate': False, 'tour...</td>\n",
       "      <td>b'False'</td>\n",
       "      <td>-81.726357</td>\n",
       "      <td>b'Hop Hing'</td>\n",
       "      <td>b'False'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'OH'</td>\n",
       "      <td>b\"u'none'\"</td>\n",
       "      <td>b'False'</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>b'North Royalton'</td>\n",
       "      <td>3.5</td>\n",
       "      <td>b'True'</td>\n",
       "      <td>41.330546</td>\n",
       "      <td>b\"u'casual'\"</td>\n",
       "      <td>b'False'</td>\n",
       "      <td>b'True'</td>\n",
       "      <td>chinese</td>\n",
       "      <td>[No, frills, Chinese, takeout, joint, serves, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 attributes.Ambience  \\\n",
       "0  b\"{'romantic': False, 'intimate': False, 'clas...   \n",
       "1  b\"{'romantic': False, 'intimate': False, 'tour...   \n",
       "2                                                NaN   \n",
       "3  b\"{'romantic': False, 'intimate': False, 'clas...   \n",
       "4  b\"{'romantic': False, 'intimate': False, 'tour...   \n",
       "\n",
       "  attributes.OutdoorSeating   longitude                     name  \\\n",
       "0                  b'False'  -81.820568              b'Rush Inn'   \n",
       "1                   b'True' -112.032893              b'GreenMix'   \n",
       "2                       NaN  -79.339163  b'BarBurrito - Gerrard'   \n",
       "3                  b'False' -115.242714              b'SalvaMex'   \n",
       "4                  b'False'  -81.726357              b'Hop Hing'   \n",
       "\n",
       "  attributes.RestaurantsReservations attributes.RestaurantsPriceRange2  \\\n",
       "0                           b'False'                              b'2'   \n",
       "1                           b'False'                              b'2'   \n",
       "2                           b'False'                               NaN   \n",
       "3                           b'False'                              b'1'   \n",
       "4                           b'False'                              b'1'   \n",
       "\n",
       "  attributes.NoiseLevel  state attributes.Alcohol attributes.HasTV  ...  \\\n",
       "0            b\"u'loud'\"  b'OH'     b\"u'full_bar'\"          b'True'  ...   \n",
       "1           b\"u'quiet'\"  b'AZ'         b\"u'none'\"         b'False'  ...   \n",
       "2                   NaN  b'ON'                NaN              NaN  ...   \n",
       "3            b\"'quiet'\"  b'NV'          b\"'none'\"          b'True'  ...   \n",
       "4                   NaN  b'OH'         b\"u'none'\"         b'False'  ...   \n",
       "\n",
       "  is_open               city  stars  attributes.RestaurantsTakeOut   latitude  \\\n",
       "0       1        b'Lakewood'    4.0                        b'True'  41.484197   \n",
       "1       1         b'Phoenix'    3.5                        b'True'  33.379283   \n",
       "2       1         b'Toronto'    3.0                       b'False'  43.669144   \n",
       "3       0       b'Las Vegas'    4.0                        b'True'  36.159527   \n",
       "4       1  b'North Royalton'    3.5                        b'True'  41.330546   \n",
       "\n",
       "   attributes.RestaurantsAttire attributes.RestaurantsDelivery  \\\n",
       "0                  b\"u'casual'\"                       b'False'   \n",
       "1                  b\"u'casual'\"                       b'False'   \n",
       "2                           NaN                            NaN   \n",
       "3                   b\"'casual'\"                        b'True'   \n",
       "4                  b\"u'casual'\"                       b'False'   \n",
       "\n",
       "   attributes.GoodForKids                   label  \\\n",
       "0                b'False'  american (traditional)   \n",
       "1                 b'True'          american (new)   \n",
       "2                     NaN                 mexican   \n",
       "3                 b'True'                 mexican   \n",
       "4                 b'True'                 chinese   \n",
       "\n",
       "                                              tokens  \n",
       "0  [So, stopped, way, Side, Quest, street, nWe, k...  \n",
       "1  [This, go, healthy, spot, The, food, always, f...  \n",
       "2  [Food, court, meal, Gerrard, Square, It, since...  \n",
       "3  [Located, Rainbow, Charleston, small, family, ...  \n",
       "4  [No, frills, Chinese, takeout, joint, serves, ...  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c55a4d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = Pipeline([\n",
    "    ('embed', CustomEmbeddingVectorizer(vocabulary, embedding_weights))\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('text', text_pipeline, 'tokens'),\n",
    "    ('cat', categorical_pipeline, categorical_cols),\n",
    "    ('num', numeric_pipeline, numeric_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa2d1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
      "Best parameters: {'clf__C': 10, 'tfidf__max_df': 0.75, 'tfidf__max_features': None, 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2)}\n",
      "Best CV score: 0.8073228720874941\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model with TF-IDF\n",
    "\n",
    "X_full = train_df[['tokens'] + categorical_cols + numeric_cols]\n",
    "X_test = test_df[['tokens'] + categorical_cols + numeric_cols]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_df[\"label\"])\n",
    "y_full = label_encoder.fit_transform(train_df[\"label\"])\n",
    "\n",
    "X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
    "    train_df['tokens'].apply(lambda x: ' '.join(x)),  # join tokens into text\n",
    "    train_df['label'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_df=0.75, min_df=3)\n",
    "\n",
    "# Define pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(max_iter=5000, solver='liblinear', class_weight='balanced'))\n",
    "])\n",
    "\n",
    "\n",
    "# TF-IDF param grid\n",
    "param_grid = {\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'tfidf__min_df': [1, 3, 5],\n",
    "    'tfidf__max_df': [0.75, 0.9, 1.0],\n",
    "    'tfidf__max_features': [None, 10000, 20000],\n",
    "    'clf__C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "grid = GridSearchCV(text_clf, param_grid, scoring='accuracy', cv=5, n_jobs=-1, verbose=2)\n",
    "grid.fit(X_train_text, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best CV score:\", grid.best_score_)\n",
    "\n",
    "# Fit and evaluate\n",
    "# text_clf.fit(X_train_text, y_train)\n",
    "# val_preds = text_clf.predict(X_val_text)\n",
    "# acc = accuracy_score(y_val, val_preds)\n",
    "\n",
    "# print(f\"Validation accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cac90fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_df=0.75, min_df=3)\n",
    "\n",
    "X_full = train_df[['tokens'] + categorical_cols + numeric_cols]\n",
    "X_test = test_df[['tokens'] + categorical_cols + numeric_cols]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "#y_train = label_encoder.fit_transform(train_df[\"label\"])\n",
    "y_full = label_encoder.fit_transform(train_df[\"label\"])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, random_state=42, stratify=y_full\n",
    ")\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    objective='multi:softmax',  # or 'multi:prob' if you want probabilities\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "cache_dir = './cache_dir'\n",
    "memory = Memory(cache_dir, verbose=0)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('tfidf', tfidf),  # fixed from Step 1\n",
    "    ('clf', xgb)\n",
    "], memory=memory)\n",
    "\n",
    "param_grid = {\n",
    "    'clf__n_estimators': [100, 200],\n",
    "    'clf__max_depth': [4, 6],\n",
    "    'clf__learning_rate': [0.05, 0.1, 0.2],\n",
    "    'clf__subsample': [0.8, 1.0],\n",
    "    'clf__colsample_bytree': [1.0]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=3, verbose=2, n_jobs=-1, error_score='raise')\n",
    "grid.fit(X_train_text, y_train)\n",
    "\n",
    "print(\"Best score:\", grid.best_score_)\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "best_model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecd198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM with TF-IDF Vectorizer\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_df=0.75, min_df=3)\n",
    "\n",
    "X_full = train_df[['tokens'] + categorical_cols + numeric_cols]\n",
    "X_test = test_df[['tokens'] + categorical_cols + numeric_cols]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "#y_train = label_encoder.fit_transform(train_df[\"label\"])\n",
    "y_full = label_encoder.fit_transform(train_df[\"label\"])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, random_state=42, stratify=y_full\n",
    ")\n",
    "\n",
    "cache_dir = './cache_dir'\n",
    "memory = Memory(cache_dir, verbose=0)\n",
    "\n",
    "lgb = LGBMClassifier(objective='multiclass', random_state=42, n_jobs=-1)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('tfidf', tfidf),\n",
    "    ('clf', lgb)\n",
    "], memory=memory)\n",
    "\n",
    "param_grid = {\n",
    "    'clf__n_estimators': [100, 200],\n",
    "    'clf__num_leaves': [31, 63],\n",
    "    'clf__learning_rate': [0.05, 0.1, 0.2],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=3, verbose=2, n_jobs=-1, error_score='raise')\n",
    "grid.fit(X_train_text, y_train)\n",
    "\n",
    "print(\"Best score:\", grid.best_score_)\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "best_model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ef1bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', text_pipeline, 'tokens'),\n",
    "        ('cat', categorical_pipeline, categorical_cols),\n",
    "        ('num', numeric_pipeline, numeric_cols)\n",
    "    ],\n",
    "    remainder='drop'  # drop any other columns not listed\n",
    ")\n",
    "\n",
    "model_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('clf', LogisticRegression(max_iter=5000, solver='liblinear'))\n",
    "])\n",
    "\n",
    "model_pipeline.fit(train_df, train_df['label'])\n",
    "\n",
    "# Predict\n",
    "preds = model_pipeline.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "48d5d497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Predictions exported to predicted.csv\n"
     ]
    }
   ],
   "source": [
    "X_full_text = train_df['tokens'].apply(lambda x: ' '.join(x))\n",
    "text_clf.fit(X_full_text, y_full)\n",
    "\n",
    "# Predict on test data\n",
    "X_test_text = test_df['tokens'].apply(lambda x: ' '.join(x))\n",
    "y_test_pred_encoded = text_clf.predict(X_test_text)\n",
    "y_test_pred = label_encoder.inverse_transform(y_test_pred_encoded)\n",
    "\n",
    "# Export to CSV\n",
    "output_path = \"predicted.csv\"\n",
    "pd.DataFrame({\n",
    "    \"Id\": range(len(y_test_pred)),\n",
    "    \"Predicted\": y_test_pred\n",
    "}).to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ Predictions exported to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a27a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 486 candidates, totalling 2430 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danny\\Anaconda3\\envs\\xgb_clean\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:40:12] WARNING: D:\\bld\\xgboost-split_1747336816412\\work\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'classifier__lr__C': 10, 'classifier__rf__max_depth': 5, 'classifier__rf__n_estimators': 200, 'classifier__xgb__learning_rate': 0.3, 'classifier__xgb__max_depth': 3, 'classifier__xgb__n_estimators': 200}\n",
      "Best Accuracy: 0.7943889681407514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danny\\Anaconda3\\envs\\xgb_clean\\lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_full = train_df[['tokens'] + categorical_cols + numeric_cols]\n",
    "X_test = test_df[['tokens'] + categorical_cols + numeric_cols]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_df[\"label\"])\n",
    "\n",
    "X_train, X_val, y_train_split, y_val_split = train_test_split(\n",
    "    X_full, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "log_clf = LogisticRegression(solver='liblinear', max_iter=1000)\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', log_clf),\n",
    "        ('rf', rf_clf),\n",
    "        ('xgb', xgb_clf)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "model_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('classifier', voting_clf)\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'classifier__lr__C': [0.1, 1, 10],\n",
    "    'classifier__rf__n_estimators': [50, 10, 200],\n",
    "    'classifier__rf__max_depth': [5, 10],\n",
    "    'classifier__xgb__max_depth': [3, 5, 7],\n",
    "    'classifier__xgb__n_estimators': [50, 100, 200],\n",
    "    'classifier__xgb__learning_rate': [0.01, 0.1, 0.3]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# GridSearchCV setup\n",
    "grid_search = GridSearchCV(\n",
    "    model_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    error_score='raise'\n",
    ")\n",
    "\n",
    "# # RandomizedSearchCV\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=model_pipeline,\n",
    "#     param_distributions=param_grid,  # same param grid but treated as distributions\n",
    "#     n_iter=30,                       # number of random parameter settings to try (adjust as needed)\n",
    "#     cv=cv,\n",
    "#     scoring='accuracy',\n",
    "#     n_jobs=-1,\n",
    "#     verbose=1,\n",
    "#     error_score='raise',\n",
    "#     random_state=42                  # for reproducibility\n",
    "# )\n",
    "\n",
    "# Run grid search\n",
    "grid_search.fit(X_train, y_train_split)\n",
    "\n",
    "# Best model and score\n",
    "print(\"Best Params:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Predict on validation set using the best estimator\n",
    "y_val_pred = grid_search.best_estimator_.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1ac80ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danny\\Anaconda3\\envs\\xgb_clean\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:40:36] WARNING: D:\\bld\\xgboost-split_1747336816412\\work\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Report:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "        american (new)       0.58      0.40      0.48       280\n",
      "american (traditional)       0.66      0.82      0.73       536\n",
      "          asian fusion       0.26      0.07      0.11        72\n",
      "        canadian (new)       0.40      0.35      0.37        97\n",
      "               chinese       0.80      0.91      0.85       339\n",
      "               italian       0.83      0.90      0.86       406\n",
      "              japanese       0.84      0.82      0.83       213\n",
      "         mediterranean       0.90      0.71      0.80       146\n",
      "               mexican       0.96      0.94      0.95       443\n",
      "                  thai       0.88      0.75      0.81        97\n",
      "\n",
      "              accuracy                           0.77      2629\n",
      "             macro avg       0.71      0.67      0.68      2629\n",
      "          weighted avg       0.76      0.77      0.76      2629\n",
      "\n",
      "Validation Accuracy: 0.7717763408139977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danny\\Anaconda3\\envs\\xgb_clean\\lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "model_pipeline.fit(X_train, y_train_split)\n",
    "\n",
    "y_val_pred = model_pipeline.predict(X_val)\n",
    "print(\"\\nValidation Report:\")\n",
    "print(classification_report(y_val_split, y_val_pred, target_names=label_encoder.classes_))\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val_split, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a0fec71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danny\\Anaconda3\\envs\\xgb_clean\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:41:09] WARNING: D:\\bld\\xgboost-split_1747336816412\\work\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\danny\\Anaconda3\\envs\\xgb_clean\\lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_pipeline.fit(X_full, y_train)\n",
    "y_test_pred_encoded = model_pipeline.predict(X_test)\n",
    "y_test_pred = label_encoder.inverse_transform(y_test_pred_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0c37bf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Predictions exported to predicted.csv\n"
     ]
    }
   ],
   "source": [
    "output_path = \"predicted.csv\"\n",
    "pd.DataFrame({\n",
    "    \"Id\": range(len(y_test_pred)),\n",
    "    \"Predicted\": y_test_pred\n",
    "}).to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ Predictions exported to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dc8704",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgb_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
